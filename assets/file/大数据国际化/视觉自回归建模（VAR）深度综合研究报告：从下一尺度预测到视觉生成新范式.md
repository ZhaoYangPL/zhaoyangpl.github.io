# 视觉自回归建模（VAR）深度综合研究报告：从下一尺度预测到视觉生成新范式



报告作者：人工智能前沿技术分析师

日期：2025年11月

主题：基于《Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction》及其衍生技术的深度剖析、扩展定律验证与学术汇报策略规划

------



## 执行摘要与汇报战略导引





### 报告背景与核心发现



本深度研究报告旨在全面解析由北京大学与字节跳动（ByteDance）联合提出的“视觉自回归建模”（Visual Autoregressive Modeling, VAR）框架 1。该研究成果在NeurIPS 2024上荣获最佳论文奖，标志着生成式计算机视觉领域的一个重要转折点 2。长期以来，视觉生成领域被扩散模型（Diffusion Models）所统治，而自回归（Autoregressive, AR）模型因计算效率低、空间结构破坏等问题处于劣势。VAR通过引入“下一尺度预测”（Next-Scale Prediction）范式，从根本上重构了图像的序列化定义，不仅在图像质量（FID 1.73）和推理速度（提升约20倍）上超越了当前最先进的扩散Transformer（DiT），更关键的是，它首次在视觉生成中清晰地验证了类似大语言模型（LLM）的幂律扩展定律（Scaling Laws）1。



### 针对学术汇报的战略建议



针对用户提出的汇报需求，本报告建议将汇报**核心方向**定位于：“**视觉生成的范式回归：从序列依赖到尺度依赖的Scaling Law验证**”。这一方向不仅涵盖了模型架构的创新，更触及了人工智能领域当前最核心的命题——多模态统一建模与神经扩展定律。

**建议汇报大纲（融入报告结构）：**

1. **痛点重述**：传统AR模型（如VQGAN）为何在视觉领域“水土不服”？（数学假设与视觉感知的冲突）。
2. **核心洞察**：人类视觉的“粗糙到精细”认知过程如何转化为“下一尺度预测”算法？
3. **理论突破**：从 $O(n^6)$ 到 $O(n^4)$ 的复杂度降维证明。
4. **实证高潮**：视觉领域的“GPT时刻”——Scaling Laws的完美拟合（相关系数 -0.998）。
5. **未来展望**：VAR家族（Infinity, InfinityStar）如何通过Bitwise建模与时空金字塔通向视频生成与AGI。

**建议搜集的关键辅助材料（将在报告中详细说明）：**

- **注意力热力图（Attention Maps）**：特别是VAR论文附录中的Figure 9，用于直观展示传统VQGAN的非因果依赖与VAR的层级因果依赖的区别 1。
- **Scaling Law拟合曲线**：必须展示Figure 5和Figure 6，强调Loss与参数量/计算量的双对数线性关系，这是证明该方法具备“未来性”的最强证据 1。
- **对比视频/GIF**：从GitHub项目页下载VAR生成的由粗到细的动态生成过程，以及InfinityStar生成的视频样本，以视觉化方式展示其与扩散模型去噪过程的差异 2。

------



## 视觉生成的历史演进与自回归的困境



要深刻理解VAR的贡献，必须首先构建一个宏大的技术背景坐标系。生成式人工智能（Generative AI）的核心任务是对数据分布 $p(x)$ 进行建模并采样。在过去的十年中，计算机视觉领域经历了多次范式转移。



### 生成对抗网络（GAN）的辉煌与局限



生成对抗网络（GANs）通过生成器与判别器的零和博弈，实现了极高的采样速度（单步生成）和较高的图像保真度。BigGAN 1 和 StyleGAN-XL 5 是这一路线的巅峰之作。然而，GAN面临着训练极其不稳定（模式坍塌）、难以覆盖全部分布（召回率低）以及在极大规模数据上扩展困难的问题。这使得研究界开始寻找更稳定、更具可扩展性的替代方案。



### 扩散模型（Diffusion Models）的统治



去噪扩散概率模型（DDPM）及其后续的潜在扩散模型（LDM/Stable Diffusion）彻底改变了这一格局。扩散模型通过将图像生成建模为一个逐步去噪的马尔可夫链，实现了对数据分布的精细覆盖，生成了前所未有的高质量图像。

特别是Diffusion Transformer (DiT) 6 的出现，将U-Net骨干网络替换为Transformer，进一步提升了模型的可扩展性，成为了Sora等视频生成模型的基础。尽管如此，扩散模型存在天然的短板：

- **推理成本高昂**：生成一张图像通常需要数十甚至上百次迭代，导致延迟高。
- **与语言模型架构割裂**：LLM主要基于离散Token的自回归预测，而扩散模型基于连续噪声预测。这种架构上的差异阻碍了视觉与语言在底层的深度统一 7。



### 自回归模型（AR）在视觉领域的“水土不服”



在NLP领域，基于Transformer的自回归模型（如GPT系列）已经证明了其统治力。其核心公式为：



$$p(x) = \prod_{t=1}^{T} p(x_t | x_{<t})$$



即当前时刻的输出仅依赖于过去时刻的输入。

为了将这一范式引入视觉，先驱工作如iGPT、VQGAN 1 采取了“削足适履”的策略：

1. **Token化（Tokenization）**：使用VQ-VAE将 $H \times W$ 的图像压缩并量化为 $h \times w$ 的离散Token矩阵。
2. **序列化（Flattening）**：使用光栅扫描（Raster-Scan）顺序（如“Z”字形或行优先）将二维矩阵拉平为一维序列。
3. **预测**：应用标准的Transformer进行Next-Token Prediction。

根本性缺陷分析：

本报告认为，这种直接迁移存在严重的理论缺陷，这也是VAR试图解决的核心问题：

- **空间结构破坏（Violation of Spatial Locality）**：将二维图像强行拉平切断了像素在垂直方向上的邻域关系。例如，像素 $(i, j)$ 与 $(i+1, j)$ 在空间上紧密相关，但在光栅扫描序列中可能相隔整整一行（$w$ 个Token）。这迫使Transformer花费大量注意力机制去“重新学习”这些本应显而易见的空间关系 1。
- **单向依赖的谬误（Unidirectional Bias）**：图像是双向感知的实体。一个像素不仅由其“左上”方的像素决定，也受其“右下”方像素的影响。传统的AR强制施加单向因果掩码，人为限制了上下文信息的利用 3。
- **计算复杂度的诅咒**：生成 $n \times n$ 的图像需要 $n^2$ 步。对于Transformer，$n^2$ 长度的序列意味着 $O((n^2)^2) = O(n^4)$ 的注意力计算量，总生成复杂度高达 $O(n^6)$（详见第4章数学推导）。这使得高分辨率生成在计算上几乎不可行。

------



## 视觉自回归建模（VAR）的理论重构



VAR提出了一种革命性的观点：**图像的“序”不应是空间位置的先后，而应是信息密度的层级（Scale）。**



### 核心定义：下一尺度预测（Next-Scale Prediction）



VAR将图像生成重新定义为从粗糙到精细（Coarse-to-Fine）的递归过程。这与人类绘画或认知的过程高度一致：先确定整体构图（轮廓、大色块），再逐步填充细节（纹理、边缘）。

数学上，假设一张图像被编码为 $K$ 个不同分辨率的Token图序列 $(r_1, r_2, \dots, r_K)$，其中 $r_1$ 分辨率最低（如 $1 \times 1$），$r_K$ 分辨率最高（如 $32 \times 32$）。VAR的联合概率分布定义为：



$$p(r_1, r_2, \dots, r_K) = \prod_{k=1}^{K} p(r_k | r_{1}, \dots, r_{k-1})$$



这里，每一个 $r_k$ 不再是一个标量Token，而是一个完整的Token图（Token Map）。在第 $k$ 步，模型依据前 $k-1$ 层的累积信息，并行地生成 $r_k$ 中的所有Token 1。



### 架构创新：多尺度残差量化（Multi-Scale Residual Quantization）



为了实现上述预测目标，VAR改进了传统的VQ-VAE。

- **传统VQ-VAE**：Image $\to$ Encoder $\to$ $z$ (Single Scale) $\to$ Quantize $\to$ Decoder $\to$ Reconstruction。

- **VAR VQ-VAE**：引入了多尺度设计。图像特征 $f$ 被下采样到多个尺度 $h_k \times w_k$。关键在于，第 $k$ 层的量化目标不是原始图像特征，而是**上一尺度插值放大后的残差（Residual）**。

  - **Algorithm 1 (Encoding)** 1:

    1. 初始化残差特征 $R = \emptyset$。

    2. 对于每个尺度 $k = 1 \dots K$：

    3. 将特征图 $f$ 插值到当前分辨率。

    4. 量化得到 $r_k$，将其加入序列 $R$。

    5. 计算量化误差，作为下一层的输入。

       这种设计确保了每一层 $r_k$ 包含的信息是相对于 $r_{<k}$ 的“增量信息”，即更高频的细节。这不仅符合信息论中的压缩逻辑，也为自回归模型提供了明确的学习目标。



### VAR Transformer的设计细节



VAR采用了标准的GPT-2类Decoder-only架构，但针对多尺度特性进行了关键调整：

- **层级因果掩码（Block-wise Causal Mask）**：这是VAR的灵魂。
  - **尺度间（Inter-scale）**：第 $k$ 层的Token只能关注 $1 \dots k$ 层的信息，严禁看到 $k+1$ 层（因果性）。
  - **尺度内（Intra-scale）**：第 $k$ 层内部的所有Token可以**互相看见**（双向注意力）。这意味着在生成某一分辨率的细节时，模型可以充分利用该分辨率下的全局上下文。这完美解决了传统AR单向依赖的问题，恢复了图像的空间局部性 1。
- **位置编码**：除了标准的2D坐标嵌入，还引入了特殊的“尺度嵌入”（Scale Embedding），告知模型当前处于生成的哪个分辨率阶段。
- **特殊Token**：使用 `[s]` (Start Token) 作为起始，同时用于注入类别条件（Class Label）等控制信号，支持Classifier-Free Guidance (CFG)。

------



## 复杂度分析与数学证明



在汇报中，展示数学证明能极大地增强技术深度。以下是关于VAR如何实现计算效率飞跃的详细推导。



### 传统AR的时间复杂度：$O(n^6)$



对于一个 $n \times n$ 的特征图，传统AR将其视为长度 $L = n^2$ 的序列。

- Transformer的自注意力机制计算复杂度与序列长度的平方成正比，即 $O(L^2)$。

- 在第 $i$ 步生成时，需要计算当前Token与之前 $i-1$ 个Token的注意力，计算量约为 $i$。

- 总计算量为所有步数的累加：

  

  $$\sum_{i=1}^{n^2} i \times d_{model} \approx \sum_{i=1}^{n^2} i^2 \quad (\text{若考虑全量矩阵运算})$$

  

  更为严谨的推导 1 指出，对于自回归生成，总操作数约为：

  

  $$\sum_{i=1}^{n^2} i^2 = \frac{1}{6} n^2 (n^2 + 1)(2n^2 + 1) \approx \frac{1}{3} n^6$$

  

  这意味着，随着分辨率 $n$ 的线性增加，计算成本呈六次方爆炸式增长。



### VAR的时间复杂度：$O(n^4)$



VAR采用多尺度并行生成。假设尺度按倍数 $a$ 增长（例如 $a=2$），则最大分辨率为 $n$。

- 总生成步数从 $n^2$ 骤降为 $K = \log_a n$。

- 在第 $k$ 步，需并行生成 $n_k \times n_k$ 个Token。此时序列总长度约为 $n_k^2$。

- 单步注意力的计算复杂度为 $O((n_k^2)^2) = O(n_k^4)$。

- 总计算量为各尺度之和：

  

  $$\sum_{k=1}^{K} (a^{k-1})^4 = \sum_{k=1}^{K} a^{4(k-1)}$$

  

  这是一个公比为 $a^4$ 的等比数列求和。当 $a > 1$ 时，总和主要由最后一项（最大分辨率）主导：

  

  $$\approx O((a^{K-1})^4) = O(n^4)$$

  

  结论：从 $O(n^6)$ 到 $O(n^4)$ 的降维，使得VAR在生成高分辨率图像时具有天然的巨大速度优势。实测数据显示，生成256x256图像，VAR比VQGAN快约20倍 1。

------



## 神经扩展定律（Scaling Laws）的实证研究



VAR论文最核心的贡献在于验证了视觉生成的Scaling Laws。这是向AGI迈进的重要标志，暗示了只要增加算力，性能就能持续提升，没有明显的“天花板”。



### 幂律关系的发现



研究团队训练了从18M到2B参数的一系列VAR模型。实验结果显示，测试集损失（Test Loss, $L$）与模型参数量（$N$）之间存在极强的对数线性关系 1。

拟合公式如下：



$$L_{last} = (2.0 \cdot N)^{-0.23}$$

$$L_{avg} = (2.5 \cdot N)^{-0.20}$$



相关系数（Pearson Correlation）高达 -0.998，这意味着数据点几乎完美地落在拟合直线上。相比之下，扩散模型（如DiT）在参数增加到一定程度（如600M以上）后，往往会出现收益递减（Diminishing Returns）的现象，性能提升趋于平缓 1。VAR的这一特性证明了自回归Transformer在视觉数据压缩与建模上的强大潜力。



### 5.2 计算最优训练（Compute-Optimal Training）



类似于LLM中的Chinchilla定律，VAR研究也探索了计算量（$C$）与性能的关系。



$$L \propto C^{-\alpha}$$



研究发现，为了达到特定的性能指标（Loss），存在一个最优的模型大小与训练数据量的配比。更大的模型更加“样本高效”（Sample Efficient），即大模型用较少的数据迭代就能达到小模型训练很久才能达到的效果。这一发现为未来训练百亿（10B+）参数级的视觉模型提供了资源分配的理论指导：应当优先扩大模型规模，而非单纯增加训练步数 9。

------



## 实验结果与对比分析：ImageNet基准测试



本节详细对比VAR与当前SOTA模型的性能。使用ImageNet 256x256 条件生成作为标准赛道。



### 定量指标对比表



下表汇总了VAR与主流GAN、Diffusion及AR模型的关键指标 1：

| **模型类别**   | **模型名称**             | **参数量** | **FID (↓) 图像质量** | **IS (↑) 多样性** | **推理速度 (相对VAR)** | **备注**     |
| -------------- | ------------------------ | ---------- | -------------------- | ----------------- | ---------------------- | ------------ |
| **GAN**        | StyleGAN-XL              | 166M       | 2.30                 | 265.1             | 0.3x (Faster)          | 极快但训练难 |
| **Diffusion**  | LDM-4 (Latent Diffusion) | 400M       | 3.60                 | 247.7             | ~31x (Slower)          | SD的基础     |
| **Diffusion**  | DiT-XL/2 (Sora基础)      | 675M       | 2.27                 | 278.2             | ~45x (Slower)          | 强劲对手     |
| **Diffusion**  | L-DiT-3B                 | 3.0B       | 2.10                 | 304.4             | >45x (Slower)          | 大规模DiT    |
| **传统AR**     | VQGAN                    | 1.4B       | 15.78                | 74.3              | ~24x (Slower)          | 旧AR范式     |
| **VAR (Ours)** | **VAR-d30**              | **2.0B**   | **1.73**             | **350.2**         | **1.0x**               | **SOTA**     |

**深度分析：**

1. **超越DiT**：VAR-d30取得了 **FID 1.73** 的成绩，这是自回归模型首次在图像质量上正面击败扩散模型（DiT-XL/2 FID 2.27）。这是一个历史性的时刻，证明了只要序列定义正确，AR完全可以生成比Diffusion更逼真的图像。
2. **多样性爆发**：Inception Score (IS) 高达350.2，远超DiT的304.4。这表明VAR生成的图像不仅真实，而且类别特征极其鲜明，覆盖了分布的各个模式。
3. **速度优势**：尽管比单步GAN慢，但VAR比同等规模的DiT快约45倍。这主要得益于其对数级的生成步数（~10步）vs 扩散模型的线性迭代步数（通常20-50步，甚至更多）。



### 消融实验（Ablation Study）深度解读



为了验证VAR各组件的有效性，研究进行了详细的消融实验 1。

- **Baseline (Vanilla AR)**: FID 18.65。
- **+ Next-Scale Prediction**: FID 降至 5.22。这是最大的性能飞跃，证明了“下一尺度”范式本身是核心贡献。
- **+ AdaLN (Adaptive LayerNorm)**: FID 降至 4.95。自适应层归一化帮助模型更好地整合类别条件信息。
- **+ Top-k Sampling**: FID 降至 4.64。优化采样策略减少了离群噪点。
- **+ CFG (Classifier-Free Guidance)**: FID 降至 3.60。如同扩散模型一样，CFG显著提升了生成内容与条件的匹配度。
- **+ Scale Up (2.0B Params)**: FID 最终降至 1.73。验证了Scaling Laws的有效性，模型越大效果越好。



### 零样本泛化任务



VAR展现了类似于GPT-3的零样本（Zero-Shot）能力，无需针对特定任务微调：

- **In-painting（图像补全）**：给定周围的Token，预测中间缺失的Token。由于VAR学习了多尺度的全局上下文，它能补全出符合逻辑的物体结构（如补全被遮挡的狗头），而不仅仅是纹理填充 1。
- **Out-painting（图像外推）**：根据图像的一部分预测其余部分，VAR展现了对图像整体布局的宏观理解。
- **Class-conditional Editing（编辑）**：改变类别条件Token，引导图像在保持结构的同时变换语义（如将猫变为狗）。

------



## VAR宇宙的爆发：Infinity 与 InfinityStar



在汇报中，展示技术的**延续性**和**生命力**至关重要。VAR并不是孤立的工作，它开启了一个新的家族。根据最新的Github和arXiv信息，VAR已经演化出了两个更强大的版本 4。



### Infinity：突破分辨率的“按位建模”



痛点：传统的VAR依赖VQ-VAE的离散词表（Codebook）。当词表大小固定（如4096）时，生成极高分辨率（如1024x1024以上）图像会导致细节丢失，因为有限的Token无法编码无限的纹理变化。

创新：Infinity模型引入了Bitwise AutoRegressive Modeling（按位自回归）。

- 它不直接预测Token索引（0-4095），而是将Token拆解为二进制位（Bits）进行预测。
- 这相当于在数学上将词表大小扩展到了指数级，极大地提升了表达能力。
- **性能**：Infinity生成1024x1024图像仅需0.8秒，比Stable Diffusion 3 Medium快2.6倍，且在GenEval和ImageReward评分上全面超越SD3和SDXL 13。它是目前世界上**最快且最强**的文本生图（T2I）模型之一。



### InfinityStar：时空金字塔与视频生成



痛点：视频生成需要同时处理空间和时间的一致性，计算量更加庞大。

创新：InfinityStar提出了Spacetime Pyramid Modeling（时空金字塔）。

- 将视频视为三维信号，不仅在空间分辨率上进行“下一尺度预测”，在时间帧率上也采用层级生成。
- **统一模型**：InfinityStar是一个全能模型，支持Text-to-Image, Text-to-Video, Image-to-Video, Video Extrapolation等所有任务。
- **性能**：在VBench上得分83.74，超越了HunyuanVideo和CogVideoX。它能生成720p的工业级视频，且速度比扩散模型快10倍 12。这一工作证明了VAR范式在视频领域的统治潜力。

------



## 批判性讨论与潜在风险



一个高质量的汇报需要展示批判性思维。我们在赞赏VAR的同时，也应指出其局限性和争议点，这会增加汇报的深度。



### 离散与连续的哲学之争



VAR的本质是将连续的图像信号强制离散化。

- **挑战**：虽然Scaling Laws表现良好，但离散Tokenizer的重构损失（Reconstruction Loss）始终是一个硬伤。无论自回归模型多强，生成的图像质量上限都被VQ-VAE锁死。如果VQ-VAE丢失了人脸微表情，VAR无法恢复。
- **对比**：扩散模型直接在连续潜空间（Latent Space）操作，理论上能保留更多细微信息。未来可能会出现结合两者优势的混合模型。



### 速度优势的实际场景考量



虽然论文宣称VAR比DiT快45倍，但这基于DiT运行250步的假设 16。

- **反驳**：在实际工业应用中，使用Turbo/Lightning等蒸馏技术的DiT只需4-8步即可生成不错的结果。此时，VAR（需要约10-15个尺度步骤）的速度优势可能会被抹平，甚至因模型参数量更大（2B vs 600M）而变慢。
- **回应**：VAR也可以应用蒸馏技术（如Speculative Decoding）进一步加速。且VAR在未经优化的基线对比中胜出，说明其架构效率下限极高。

------



## 结论



**Visual Autoregressive Modeling (VAR)** 不仅仅是一个新算法，它代表了计算机视觉领域的一次认知觉醒。它告诉我们：

1. **顺序很重要**：图像生成的最佳路径不是从左到右，而是从粗到细。
2. **统一是趋势**：通过VAR，视觉生成终于可以说着和GPT一样的“语言”（自回归），这为构建真正的多模态通用大模型扫清了障碍。
3. **规模即正义**：Scaling Laws在视觉领域的验证，预示着未来视觉模型的发展将进入“拼算力、拼规模”的可预测快车道。

对于本次汇报，建议最后以一张**技术路线图**收尾：从VQGAN的探索，到VAR的突破，再到Infinity/InfinityStar的爆发，展示这一技术流派如何一步步重塑我们对视觉生成的理解。这不仅能展示你们团队对文献的透彻理解，也能体现对前沿趋势的敏锐洞察。

